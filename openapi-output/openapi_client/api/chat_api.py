# coding: utf-8

"""
    Eden AI

    Your project description

    The version of the OpenAPI document: 2.0
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501

import warnings
from pydantic import validate_call, Field, StrictFloat, StrictStr, StrictInt
from typing import Any, Dict, List, Optional, Tuple, Union
from typing_extensions import Annotated

from openapi_client.models.textchat_chat_request import TextchatChatRequest
from openapi_client.models.textchat_chat_stream_request import TextchatChatStreamRequest
from openapi_client.models.textchat_response_model import TextchatResponseModel

from openapi_client.api_client import ApiClient, RequestSerialized
from openapi_client.api_response import ApiResponse
from openapi_client.rest import RESTResponseType


class ChatApi:
    """NOTE: This class is auto generated by OpenAPI Generator
    Ref: https://openapi-generator.tech

    Do not edit the class manually.
    """

    def __init__(self, api_client=None) -> None:
        if api_client is None:
            api_client = ApiClient.get_default()
        self.api_client = api_client


    @validate_call
    async def text_text_chat_create(
        self,
        textchat_chat_request: TextchatChatRequest,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)],
                Annotated[StrictFloat, Field(gt=0)]
            ]
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> TextchatResponseModel:
        """Chat

        <details><summary><strong style='color: #0072a3; cursor: pointer'>Available Providers</strong></summary>    |Provider|Model|Version|Price|Billing unit| |----|----|-------|-----|------------| |**openai**|**gpt-3.5-turbo**|`v1Beta`|0.002 (per 1000 token)|1 token |**openai**|**gpt-3.5-turbo-1106**|`v1Beta`|0.002 (per 1000 token)|1 token |**openai**|-|`v1Beta`|0.002 (per 1000 token)|1 token |**openai**|**gpt-4**|`v1Beta`|0.06 (per 1000 token)|1 token |**openai**|**gpt-3.5-turbo-0301**|`v1Beta`|0.002 (per 1000 token)|1 token |**openai**|**gpt-4-0314**|`v1Beta`|0.06 (per 1000 token)|1 token |**openai**|**gpt-3.5-turbo-16k**|`v1Beta`|0.004 (per 1000 token)|1 token |**openai**|**gpt-4-1106-preview**|`v1Beta`|0.03 (per 1000 token)|1 token |**openai**|**gpt-4-vision-preview**|`v1Beta`|0.03 (per 1000 token)|1 token |**google**|-|`v1`|0.5 (per 1000000 char)|1000 char |**replicate**|-|`v1`|0.0032 (per 1 exec_time)|1 exec_time |**cohere**|**command**|`2022-12-06`|2.0 (per 1000000 token)|1 token |**cohere**|**command-light**|`2022-12-06`|0.6 (per 1000000 token)|1 token |**cohere**|**command-light-nightly**|`2022-12-06`|0.6 (per 1000000 token)|1 token |**cohere**|**command-nightly**|`2022-12-06`|2.0 (per 1000000 token)|1 token |**cohere**|-|`2022-12-06`|2.0 (per 1000000 token)|1 token |**meta**|-|`v1`|2.56 (per 1000000 token)|1 token |**meta**|**llama2-13b-chat-v1**|`v1`|1.0 (per 1000000 token)|1 token |**meta**|**llama2-70b-chat-v1**|`v1`|2.56 (per 1000000 token)|1 token |**mistral**|-|`v0.0.1`|0.42 (per 1000000 token)|1 token |**mistral**|**mistral-medium**|`v0.0.1`|8.1 (per 1000000 token)|1 token |**mistral**|**mistral-small**|`v0.0.1`|6.0 (per 1000000 token)|1 token |**mistral**|**mistral-tiny**|`v0.0.1`|0.42 (per 1000000 token)|1 token |**perplexityai**|**mistral-7b-instruct**|`v1.0`|0.28 (per 1000000 token)|1 token |**perplexityai**|**mixtral-8x7b-instruct**|`v1.0`|0.28 (per 1000000 token)|1 token |**perplexityai**|**pplx-7b-chat**|`v1.0`|0.28 (per 1000000 token)|1 token |**perplexityai**|**pplx-7b-online**|`v1.0`|0.28 (per 1000000 token)|1 token |**perplexityai**|**codellama-34b-instruct**|`v1.0`|1.4 (per 1000000 token)|1 token |**perplexityai**|**llama-2-70b-chat**|`v1.0`|2.8 (per 1000000 token)|1 token |**perplexityai**|**pplx-70b-chat**|`v1.0`|2.8 (per 1000000 token)|1 token |**perplexityai**|**pplx-70b-online**|`v1.0`|2.8 (per 1000000 token)|1 token |**perplexityai**|-|`v1.0`|2.8 (per 1000000 token)|1 token |**anthropic**|-|`bedrock-2023-05-31`|15.0 (per 1000000 token)|1 token |**anthropic**|**claude-3-sonnet-20240229-v1:0**|`bedrock-2023-05-31`|15.0 (per 1000000 token)|1 token |**anthropic**|**claude-instant-v1**|`bedrock-2023-05-31`|2.4 (per 1000000 token)|1 token |**anthropic**|**claude-v2**|`bedrock-2023-05-31`|24.0 (per 1000000 token)|1 token |**anthropic**|**claude-3-haiku-20240307-v1:0**|`bedrock-2023-05-31`|1.25 (per 1000000 token)|1 token   </details>  <details><summary>Supported Detailed Languages</summary>      |Name|Value| |----|-----| |**Auto detection**|`auto-detect`|  </details><details><summary>Supported Models</summary><details><summary>openai</summary>      |Name|Value| |----|-----| |**openai**|`gpt-3.5-turbo`| ||`gpt-3.5-turbo-0301`| ||`gpt-3.5-turbo-1106`| ||`gpt-3.5-turbo-16k`| ||`gpt-4`| ||`gpt-4-0314`| ||`gpt-4-1106-preview`| ||`gpt-4-vision-preview`|  </details><details><summary>google</summary>      |Name|Value| |----|-----| |**google**|`chat-bison`|  </details><details><summary>replicate</summary>      |Name|Value| |----|-----| |**replicate**|`llama-2-70b-chat`|  </details><details><summary>cohere</summary>      |Name|Value| |----|-----| |**cohere**|`command`| ||`command-light`| ||`command-light-nightly`| ||`command-nightly`|  </details><details><summary>meta</summary>      |Name|Value| |----|-----| |**meta**|`llama2-13b-chat-v1`| ||`llama2-70b-chat-v1`|  </details><details><summary>mistral</summary>      |Name|Value| |----|-----| |**mistral**|`large-latest`| ||`medium`| ||`small`| ||`tiny`|  </details><details><summary>perplexityai</summary>      |Name|Value| |----|-----| |**perplexityai**|`codellama-34b-instruct`| ||`llama-2-70b-chat`| ||`mistral-7b-instruct`| ||`mixtral-8x7b-instruct`| ||`pplx-70b-chat`| ||`pplx-70b-online`| ||`pplx-7b-chat`| ||`pplx-7b-online`|  </details><details><summary>anthropic</summary>      |Name|Value| |----|-----| |**anthropic**|`claude-3-sonnet-20240229-v1:0`| ||`claude-instant-v1`| ||`claude-v2`|  </details>  </details>

        :param textchat_chat_request: (required)
        :type textchat_chat_request: TextchatChatRequest
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """ # noqa: E501

        _param = self._text_text_chat_create_serialize(
            textchat_chat_request=textchat_chat_request,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index
        )

        _response_types_map: Dict[str, Optional[str]] = {
            '200': "TextchatResponseModel",
            '400': "BadRequest",
            '500': "Error",
            '403': "Error",
            '404': "NotFoundResponse",
        }
        response_data = await self.api_client.call_api(
            *_param,
            _request_timeout=_request_timeout
        )
        await response_data.read()
        return self.api_client.response_deserialize(
            response_data=response_data,
            response_types_map=_response_types_map,
        ).data


    @validate_call
    async def text_text_chat_create_with_http_info(
        self,
        textchat_chat_request: TextchatChatRequest,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)],
                Annotated[StrictFloat, Field(gt=0)]
            ]
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> ApiResponse[TextchatResponseModel]:
        """Chat

        <details><summary><strong style='color: #0072a3; cursor: pointer'>Available Providers</strong></summary>    |Provider|Model|Version|Price|Billing unit| |----|----|-------|-----|------------| |**openai**|**gpt-3.5-turbo**|`v1Beta`|0.002 (per 1000 token)|1 token |**openai**|**gpt-3.5-turbo-1106**|`v1Beta`|0.002 (per 1000 token)|1 token |**openai**|-|`v1Beta`|0.002 (per 1000 token)|1 token |**openai**|**gpt-4**|`v1Beta`|0.06 (per 1000 token)|1 token |**openai**|**gpt-3.5-turbo-0301**|`v1Beta`|0.002 (per 1000 token)|1 token |**openai**|**gpt-4-0314**|`v1Beta`|0.06 (per 1000 token)|1 token |**openai**|**gpt-3.5-turbo-16k**|`v1Beta`|0.004 (per 1000 token)|1 token |**openai**|**gpt-4-1106-preview**|`v1Beta`|0.03 (per 1000 token)|1 token |**openai**|**gpt-4-vision-preview**|`v1Beta`|0.03 (per 1000 token)|1 token |**google**|-|`v1`|0.5 (per 1000000 char)|1000 char |**replicate**|-|`v1`|0.0032 (per 1 exec_time)|1 exec_time |**cohere**|**command**|`2022-12-06`|2.0 (per 1000000 token)|1 token |**cohere**|**command-light**|`2022-12-06`|0.6 (per 1000000 token)|1 token |**cohere**|**command-light-nightly**|`2022-12-06`|0.6 (per 1000000 token)|1 token |**cohere**|**command-nightly**|`2022-12-06`|2.0 (per 1000000 token)|1 token |**cohere**|-|`2022-12-06`|2.0 (per 1000000 token)|1 token |**meta**|-|`v1`|2.56 (per 1000000 token)|1 token |**meta**|**llama2-13b-chat-v1**|`v1`|1.0 (per 1000000 token)|1 token |**meta**|**llama2-70b-chat-v1**|`v1`|2.56 (per 1000000 token)|1 token |**mistral**|-|`v0.0.1`|0.42 (per 1000000 token)|1 token |**mistral**|**mistral-medium**|`v0.0.1`|8.1 (per 1000000 token)|1 token |**mistral**|**mistral-small**|`v0.0.1`|6.0 (per 1000000 token)|1 token |**mistral**|**mistral-tiny**|`v0.0.1`|0.42 (per 1000000 token)|1 token |**perplexityai**|**mistral-7b-instruct**|`v1.0`|0.28 (per 1000000 token)|1 token |**perplexityai**|**mixtral-8x7b-instruct**|`v1.0`|0.28 (per 1000000 token)|1 token |**perplexityai**|**pplx-7b-chat**|`v1.0`|0.28 (per 1000000 token)|1 token |**perplexityai**|**pplx-7b-online**|`v1.0`|0.28 (per 1000000 token)|1 token |**perplexityai**|**codellama-34b-instruct**|`v1.0`|1.4 (per 1000000 token)|1 token |**perplexityai**|**llama-2-70b-chat**|`v1.0`|2.8 (per 1000000 token)|1 token |**perplexityai**|**pplx-70b-chat**|`v1.0`|2.8 (per 1000000 token)|1 token |**perplexityai**|**pplx-70b-online**|`v1.0`|2.8 (per 1000000 token)|1 token |**perplexityai**|-|`v1.0`|2.8 (per 1000000 token)|1 token |**anthropic**|-|`bedrock-2023-05-31`|15.0 (per 1000000 token)|1 token |**anthropic**|**claude-3-sonnet-20240229-v1:0**|`bedrock-2023-05-31`|15.0 (per 1000000 token)|1 token |**anthropic**|**claude-instant-v1**|`bedrock-2023-05-31`|2.4 (per 1000000 token)|1 token |**anthropic**|**claude-v2**|`bedrock-2023-05-31`|24.0 (per 1000000 token)|1 token |**anthropic**|**claude-3-haiku-20240307-v1:0**|`bedrock-2023-05-31`|1.25 (per 1000000 token)|1 token   </details>  <details><summary>Supported Detailed Languages</summary>      |Name|Value| |----|-----| |**Auto detection**|`auto-detect`|  </details><details><summary>Supported Models</summary><details><summary>openai</summary>      |Name|Value| |----|-----| |**openai**|`gpt-3.5-turbo`| ||`gpt-3.5-turbo-0301`| ||`gpt-3.5-turbo-1106`| ||`gpt-3.5-turbo-16k`| ||`gpt-4`| ||`gpt-4-0314`| ||`gpt-4-1106-preview`| ||`gpt-4-vision-preview`|  </details><details><summary>google</summary>      |Name|Value| |----|-----| |**google**|`chat-bison`|  </details><details><summary>replicate</summary>      |Name|Value| |----|-----| |**replicate**|`llama-2-70b-chat`|  </details><details><summary>cohere</summary>      |Name|Value| |----|-----| |**cohere**|`command`| ||`command-light`| ||`command-light-nightly`| ||`command-nightly`|  </details><details><summary>meta</summary>      |Name|Value| |----|-----| |**meta**|`llama2-13b-chat-v1`| ||`llama2-70b-chat-v1`|  </details><details><summary>mistral</summary>      |Name|Value| |----|-----| |**mistral**|`large-latest`| ||`medium`| ||`small`| ||`tiny`|  </details><details><summary>perplexityai</summary>      |Name|Value| |----|-----| |**perplexityai**|`codellama-34b-instruct`| ||`llama-2-70b-chat`| ||`mistral-7b-instruct`| ||`mixtral-8x7b-instruct`| ||`pplx-70b-chat`| ||`pplx-70b-online`| ||`pplx-7b-chat`| ||`pplx-7b-online`|  </details><details><summary>anthropic</summary>      |Name|Value| |----|-----| |**anthropic**|`claude-3-sonnet-20240229-v1:0`| ||`claude-instant-v1`| ||`claude-v2`|  </details>  </details>

        :param textchat_chat_request: (required)
        :type textchat_chat_request: TextchatChatRequest
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """ # noqa: E501

        _param = self._text_text_chat_create_serialize(
            textchat_chat_request=textchat_chat_request,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index
        )

        _response_types_map: Dict[str, Optional[str]] = {
            '200': "TextchatResponseModel",
            '400': "BadRequest",
            '500': "Error",
            '403': "Error",
            '404': "NotFoundResponse",
        }
        response_data = await self.api_client.call_api(
            *_param,
            _request_timeout=_request_timeout
        )
        await response_data.read()
        return self.api_client.response_deserialize(
            response_data=response_data,
            response_types_map=_response_types_map,
        )


    @validate_call
    async def text_text_chat_create_without_preload_content(
        self,
        textchat_chat_request: TextchatChatRequest,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)],
                Annotated[StrictFloat, Field(gt=0)]
            ]
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> RESTResponseType:
        """Chat

        <details><summary><strong style='color: #0072a3; cursor: pointer'>Available Providers</strong></summary>    |Provider|Model|Version|Price|Billing unit| |----|----|-------|-----|------------| |**openai**|**gpt-3.5-turbo**|`v1Beta`|0.002 (per 1000 token)|1 token |**openai**|**gpt-3.5-turbo-1106**|`v1Beta`|0.002 (per 1000 token)|1 token |**openai**|-|`v1Beta`|0.002 (per 1000 token)|1 token |**openai**|**gpt-4**|`v1Beta`|0.06 (per 1000 token)|1 token |**openai**|**gpt-3.5-turbo-0301**|`v1Beta`|0.002 (per 1000 token)|1 token |**openai**|**gpt-4-0314**|`v1Beta`|0.06 (per 1000 token)|1 token |**openai**|**gpt-3.5-turbo-16k**|`v1Beta`|0.004 (per 1000 token)|1 token |**openai**|**gpt-4-1106-preview**|`v1Beta`|0.03 (per 1000 token)|1 token |**openai**|**gpt-4-vision-preview**|`v1Beta`|0.03 (per 1000 token)|1 token |**google**|-|`v1`|0.5 (per 1000000 char)|1000 char |**replicate**|-|`v1`|0.0032 (per 1 exec_time)|1 exec_time |**cohere**|**command**|`2022-12-06`|2.0 (per 1000000 token)|1 token |**cohere**|**command-light**|`2022-12-06`|0.6 (per 1000000 token)|1 token |**cohere**|**command-light-nightly**|`2022-12-06`|0.6 (per 1000000 token)|1 token |**cohere**|**command-nightly**|`2022-12-06`|2.0 (per 1000000 token)|1 token |**cohere**|-|`2022-12-06`|2.0 (per 1000000 token)|1 token |**meta**|-|`v1`|2.56 (per 1000000 token)|1 token |**meta**|**llama2-13b-chat-v1**|`v1`|1.0 (per 1000000 token)|1 token |**meta**|**llama2-70b-chat-v1**|`v1`|2.56 (per 1000000 token)|1 token |**mistral**|-|`v0.0.1`|0.42 (per 1000000 token)|1 token |**mistral**|**mistral-medium**|`v0.0.1`|8.1 (per 1000000 token)|1 token |**mistral**|**mistral-small**|`v0.0.1`|6.0 (per 1000000 token)|1 token |**mistral**|**mistral-tiny**|`v0.0.1`|0.42 (per 1000000 token)|1 token |**perplexityai**|**mistral-7b-instruct**|`v1.0`|0.28 (per 1000000 token)|1 token |**perplexityai**|**mixtral-8x7b-instruct**|`v1.0`|0.28 (per 1000000 token)|1 token |**perplexityai**|**pplx-7b-chat**|`v1.0`|0.28 (per 1000000 token)|1 token |**perplexityai**|**pplx-7b-online**|`v1.0`|0.28 (per 1000000 token)|1 token |**perplexityai**|**codellama-34b-instruct**|`v1.0`|1.4 (per 1000000 token)|1 token |**perplexityai**|**llama-2-70b-chat**|`v1.0`|2.8 (per 1000000 token)|1 token |**perplexityai**|**pplx-70b-chat**|`v1.0`|2.8 (per 1000000 token)|1 token |**perplexityai**|**pplx-70b-online**|`v1.0`|2.8 (per 1000000 token)|1 token |**perplexityai**|-|`v1.0`|2.8 (per 1000000 token)|1 token |**anthropic**|-|`bedrock-2023-05-31`|15.0 (per 1000000 token)|1 token |**anthropic**|**claude-3-sonnet-20240229-v1:0**|`bedrock-2023-05-31`|15.0 (per 1000000 token)|1 token |**anthropic**|**claude-instant-v1**|`bedrock-2023-05-31`|2.4 (per 1000000 token)|1 token |**anthropic**|**claude-v2**|`bedrock-2023-05-31`|24.0 (per 1000000 token)|1 token |**anthropic**|**claude-3-haiku-20240307-v1:0**|`bedrock-2023-05-31`|1.25 (per 1000000 token)|1 token   </details>  <details><summary>Supported Detailed Languages</summary>      |Name|Value| |----|-----| |**Auto detection**|`auto-detect`|  </details><details><summary>Supported Models</summary><details><summary>openai</summary>      |Name|Value| |----|-----| |**openai**|`gpt-3.5-turbo`| ||`gpt-3.5-turbo-0301`| ||`gpt-3.5-turbo-1106`| ||`gpt-3.5-turbo-16k`| ||`gpt-4`| ||`gpt-4-0314`| ||`gpt-4-1106-preview`| ||`gpt-4-vision-preview`|  </details><details><summary>google</summary>      |Name|Value| |----|-----| |**google**|`chat-bison`|  </details><details><summary>replicate</summary>      |Name|Value| |----|-----| |**replicate**|`llama-2-70b-chat`|  </details><details><summary>cohere</summary>      |Name|Value| |----|-----| |**cohere**|`command`| ||`command-light`| ||`command-light-nightly`| ||`command-nightly`|  </details><details><summary>meta</summary>      |Name|Value| |----|-----| |**meta**|`llama2-13b-chat-v1`| ||`llama2-70b-chat-v1`|  </details><details><summary>mistral</summary>      |Name|Value| |----|-----| |**mistral**|`large-latest`| ||`medium`| ||`small`| ||`tiny`|  </details><details><summary>perplexityai</summary>      |Name|Value| |----|-----| |**perplexityai**|`codellama-34b-instruct`| ||`llama-2-70b-chat`| ||`mistral-7b-instruct`| ||`mixtral-8x7b-instruct`| ||`pplx-70b-chat`| ||`pplx-70b-online`| ||`pplx-7b-chat`| ||`pplx-7b-online`|  </details><details><summary>anthropic</summary>      |Name|Value| |----|-----| |**anthropic**|`claude-3-sonnet-20240229-v1:0`| ||`claude-instant-v1`| ||`claude-v2`|  </details>  </details>

        :param textchat_chat_request: (required)
        :type textchat_chat_request: TextchatChatRequest
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """ # noqa: E501

        _param = self._text_text_chat_create_serialize(
            textchat_chat_request=textchat_chat_request,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index
        )

        _response_types_map: Dict[str, Optional[str]] = {
            '200': "TextchatResponseModel",
            '400': "BadRequest",
            '500': "Error",
            '403': "Error",
            '404': "NotFoundResponse",
        }
        response_data = await self.api_client.call_api(
            *_param,
            _request_timeout=_request_timeout
        )
        return response_data.response


    def _text_text_chat_create_serialize(
        self,
        textchat_chat_request,
        _request_auth,
        _content_type,
        _headers,
        _host_index,
    ) -> RequestSerialized:

        _host = None

        _collection_formats: Dict[str, str] = {
        }

        _path_params: Dict[str, str] = {}
        _query_params: List[Tuple[str, str]] = []
        _header_params: Dict[str, Optional[str]] = _headers or {}
        _form_params: List[Tuple[str, str]] = []
        _files: Dict[str, str] = {}
        _body_params: Optional[bytes] = None

        # process the path parameters
        # process the query parameters
        # process the header parameters
        # process the form parameters
        # process the body parameter
        if textchat_chat_request is not None:
            _body_params = textchat_chat_request


        # set the HTTP header `Accept`
        _header_params['Accept'] = self.api_client.select_header_accept(
            [
                'application/json'
            ]
        )

        # set the HTTP header `Content-Type`
        if _content_type:
            _header_params['Content-Type'] = _content_type
        else:
            _default_content_type = (
                self.api_client.select_header_content_type(
                    [
                        'application/json'
                    ]
                )
            )
            if _default_content_type is not None:
                _header_params['Content-Type'] = _default_content_type

        # authentication setting
        _auth_settings: List[str] = [
            'FeatureApiAuth'
        ]

        return self.api_client.param_serialize(
            method='POST',
            resource_path='/text/chat',
            path_params=_path_params,
            query_params=_query_params,
            header_params=_header_params,
            body=_body_params,
            post_params=_form_params,
            files=_files,
            auth_settings=_auth_settings,
            collection_formats=_collection_formats,
            _host=_host,
            _request_auth=_request_auth
        )




    @validate_call
    async def text_text_chat_stream_create(
        self,
        textchat_chat_stream_request: TextchatChatStreamRequest,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)],
                Annotated[StrictFloat, Field(gt=0)]
            ]
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> str:
        """Chat Stream

        Streamed version of Chat feature, the raw text will be streamed chunk by chunk.  NOTE: For this feature, you an only request one provider at a time.

        :param textchat_chat_stream_request: (required)
        :type textchat_chat_stream_request: TextchatChatStreamRequest
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """ # noqa: E501

        _param = self._text_text_chat_stream_create_serialize(
            textchat_chat_stream_request=textchat_chat_stream_request,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index
        )

        _response_types_map: Dict[str, Optional[str]] = {
            '200': "str",
        }
        response_data = await self.api_client.call_api(
            *_param,
            _request_timeout=_request_timeout
        )
        await response_data.read()
        return self.api_client.response_deserialize(
            response_data=response_data,
            response_types_map=_response_types_map,
        ).data


    @validate_call
    async def text_text_chat_stream_create_with_http_info(
        self,
        textchat_chat_stream_request: TextchatChatStreamRequest,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)],
                Annotated[StrictFloat, Field(gt=0)]
            ]
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> ApiResponse[str]:
        """Chat Stream

        Streamed version of Chat feature, the raw text will be streamed chunk by chunk.  NOTE: For this feature, you an only request one provider at a time.

        :param textchat_chat_stream_request: (required)
        :type textchat_chat_stream_request: TextchatChatStreamRequest
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """ # noqa: E501

        _param = self._text_text_chat_stream_create_serialize(
            textchat_chat_stream_request=textchat_chat_stream_request,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index
        )

        _response_types_map: Dict[str, Optional[str]] = {
            '200': "str",
        }
        response_data = await self.api_client.call_api(
            *_param,
            _request_timeout=_request_timeout
        )
        await response_data.read()
        return self.api_client.response_deserialize(
            response_data=response_data,
            response_types_map=_response_types_map,
        )


    @validate_call
    async def text_text_chat_stream_create_without_preload_content(
        self,
        textchat_chat_stream_request: TextchatChatStreamRequest,
        _request_timeout: Union[
            None,
            Annotated[StrictFloat, Field(gt=0)],
            Tuple[
                Annotated[StrictFloat, Field(gt=0)],
                Annotated[StrictFloat, Field(gt=0)]
            ]
        ] = None,
        _request_auth: Optional[Dict[StrictStr, Any]] = None,
        _content_type: Optional[StrictStr] = None,
        _headers: Optional[Dict[StrictStr, Any]] = None,
        _host_index: Annotated[StrictInt, Field(ge=0, le=0)] = 0,
    ) -> RESTResponseType:
        """Chat Stream

        Streamed version of Chat feature, the raw text will be streamed chunk by chunk.  NOTE: For this feature, you an only request one provider at a time.

        :param textchat_chat_stream_request: (required)
        :type textchat_chat_stream_request: TextchatChatStreamRequest
        :param _request_timeout: timeout setting for this request. If one
                                 number provided, it will be total request
                                 timeout. It can also be a pair (tuple) of
                                 (connection, read) timeouts.
        :type _request_timeout: int, tuple(int, int), optional
        :param _request_auth: set to override the auth_settings for an a single
                              request; this effectively ignores the
                              authentication in the spec for a single request.
        :type _request_auth: dict, optional
        :param _content_type: force content-type for the request.
        :type _content_type: str, Optional
        :param _headers: set to override the headers for a single
                         request; this effectively ignores the headers
                         in the spec for a single request.
        :type _headers: dict, optional
        :param _host_index: set to override the host_index for a single
                            request; this effectively ignores the host_index
                            in the spec for a single request.
        :type _host_index: int, optional
        :return: Returns the result object.
        """ # noqa: E501

        _param = self._text_text_chat_stream_create_serialize(
            textchat_chat_stream_request=textchat_chat_stream_request,
            _request_auth=_request_auth,
            _content_type=_content_type,
            _headers=_headers,
            _host_index=_host_index
        )

        _response_types_map: Dict[str, Optional[str]] = {
            '200': "str",
        }
        response_data = await self.api_client.call_api(
            *_param,
            _request_timeout=_request_timeout
        )
        return response_data.response


    def _text_text_chat_stream_create_serialize(
        self,
        textchat_chat_stream_request,
        _request_auth,
        _content_type,
        _headers,
        _host_index,
    ) -> RequestSerialized:

        _host = None

        _collection_formats: Dict[str, str] = {
        }

        _path_params: Dict[str, str] = {}
        _query_params: List[Tuple[str, str]] = []
        _header_params: Dict[str, Optional[str]] = _headers or {}
        _form_params: List[Tuple[str, str]] = []
        _files: Dict[str, str] = {}
        _body_params: Optional[bytes] = None

        # process the path parameters
        # process the query parameters
        # process the header parameters
        # process the form parameters
        # process the body parameter
        if textchat_chat_stream_request is not None:
            _body_params = textchat_chat_stream_request


        # set the HTTP header `Accept`
        _header_params['Accept'] = self.api_client.select_header_accept(
            [
                'text/plain'
            ]
        )

        # set the HTTP header `Content-Type`
        if _content_type:
            _header_params['Content-Type'] = _content_type
        else:
            _default_content_type = (
                self.api_client.select_header_content_type(
                    [
                        'application/json'
                    ]
                )
            )
            if _default_content_type is not None:
                _header_params['Content-Type'] = _default_content_type

        # authentication setting
        _auth_settings: List[str] = [
            'FeatureApiAuth'
        ]

        return self.api_client.param_serialize(
            method='POST',
            resource_path='/text/chat/stream',
            path_params=_path_params,
            query_params=_query_params,
            header_params=_header_params,
            body=_body_params,
            post_params=_form_params,
            files=_files,
            auth_settings=_auth_settings,
            collection_formats=_collection_formats,
            _host=_host,
            _request_auth=_request_auth
        )


